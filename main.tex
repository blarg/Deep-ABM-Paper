%\documentclass[prepare]{acmconf} % Need to Replace with SigConf Style
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2.6cm]{geometry}
\usepackage{graphicx}
\usepackage{subcaption}

%\ConferenceName{The Computational Social Science Conference 2018}
%\ConferenceShortName{CSS 2018}
%\CopyrightText{\copyright 2018. Too big a block.}

\title{Deep Learning to Facilitate Policy-Making in Agents}
\author{Kevin Andrew \\
		University of Vermont\\
		kandrew6@uvm.edu
		\and Co
	%\Author{Kevin Andrew}\\
	%\Address{University of Vermont}\\
	%\Email{kandrew6@uvm.edu}
	%\and
	%\Author{Oth}\\
	%\Address{OthAddr}\\
	%\Email{OthEmail}
}
\date{June 28, 2018; Version 0.1}

\begin{document}
\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}

The simulation of human decision-making is a key component in the
modeling of social systems on any scale.
Historically, this simulation has been done in agent-based systems
by handcrafting equations for agent behavior from factors believed
to influence agent action.

Within any model of real phenomena, simplifying assumptions must be made;
however, each assumption adds a layer of abstraction to the model
and a risk that some crucial factor of state not believed to contribute
to agent decision-making but critical to the emergent behavior being
studied is left out of the decision-making policy.

One approach to solving this problem is to have agents create
their own decision policies by learning what factors of their
states influence their perceived and actual rewards using
reinforcement learning.
Over the past decade, machine learning techniques have rapidly
developed and

In particular, Deep Double Q-learning Networks (DDQN) have shown
great potential for solving decision-making problems in real-valued
decision space \cite{lillicrap15}, usage.

In this paper, we attempt to illustrate how deep reinforcement learning
methods can be used to train agent behavior within the framework of
a larger model.
We do this by simulating a heterogeneous population of farmers on a model
landscape.
Farmers make the decision to expand, hold, or contract their farming
operations each model year by some real factor and have the ability to
implement a best management practice (BMP) to reduce the ecological impact
of their farm at some cost.
This model will be described in further detail in the next section along
with details on experimental testing.

\section{The Model}

The model used to implement 

480 parcels of land in the area of St. Albans, Vermont.

The model is based off the region of St. Albans, Vermont with
parcel data taken from [[XXX]].

Each agent's state consists of 15 values.
The actions of each agent have 5 components.
In each time-step, the agents output

Each agent can be assigned a strategy: random, greedy, or altruistic
Used to affect the reward/loss of actions

The production functions for each product were taken from [[YYY]].

\subsection{Agent Learning}

\begin{table}
\caption{Learning Parameters}
\label{tab:lparams}
\centering
\begin{tabular}{l c l}
Parameter & & Value \\\hline
Activation & & ReLU \\
Maximum Memory & & 10000 transitions \\
Action-Replay Size & & 16 transitions \\
Learning Rate (actor) & $\alpha_\mu$ & 0.005 \\
Learning Rate (critic) & $\alpha_Q$ & 0.001 \\
Future Discount & $\gamma$ & 0.9 \\
Transfer Rate & $\tau$ & 0.0001 \\
\end{tabular}
\end{table}

Using the DDQN model, each agent in the network has four neural nets
that facilitate decision-policy--making:
an actor network, a critic network, a target actor network, and a target
critic network.
Each actor network has 15 input nodes, 5 hidden layers with 15 hidden nodes
each, and 5 output nodes.
Each critic network has 18 input nodes, 5 hidden layers with 15 hidden nodes
each, and 1 output node.
Actor networks were instantiated with learning rate $\alpha_\mu = 0.005$,
whereas critic networks were instantiated with a learning rate 
$\alpha_Q = 0.001$.
For each network, He initialization was used to set network weights,
and ReLU was used for activation.

At any time the maximum number of state-transitions stored in each 
agent's memory was 10000,
with a maximum of 16 state-transitions being used for determining
the policy gradient of the agents. 
At the end of each episode, target networks were updated with transfer
rate $\tau = 0.0001$.

A summary of these learning parameters is listed in 
Table~\ref{tab:lparams}.

\subsection{Experimental Design}

Training:

500 40-year episodes for each decision type

Memory Size: 10000

Action-Replay: 16

N model runs under each strategy. approx 400 agents.

The average agent behavior and variance was tracked and can be seen in
the following section.

These agents were tested with three different reward functions:
a `greedy' function that rewarded maximizing profit,
a `nongreedy' function that rewarded minimizing environmental effects,
and a random reward function to serve as a control.

\section{Results}
\label{sec:results}

The results can be seen plotted in Figure~\ref{fig:results}.

\begin{figure}
\caption{Results}
\label{fig:results}
\begin{subfigure}{0.48\textwidth}
\caption{GREEDY}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
\caption{ALTRUISTIC}
\end{subfigure}

\begin{subfigure}{0.48\textwidth}
\caption{RANDOM}
\end{subfigure}
\end{figure}

\section{Discussion}

Base Discussion

Statistical Analysis comparing targeted behavior to random

\section{Future Work}

Implementing Extreme Events, Government Policy Factors, and Other 
Externalities

\bibliographystyle{plain}
\bibliography{sources}

\appendix
\section{ODD}
\label{apx:odd}

The ODD Should Go Here?

\end{document}
\bye
